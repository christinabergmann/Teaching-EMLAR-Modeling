{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This IPython notebook contains the code written during the computational model tutorial at EMLAR 2017, and additional explanations. \n",
    "\n",
    "- What does the model do?\n",
    "\n",
    "This model is used to test the (simplified) transitional probability hypothesis that infants use transitional probabilities to learn where word boundaries lies within unsegmented speech. The model will be written in Python, but the same ideas could also be implemented using a different programming language.\n",
    "\n",
    "- What do we need?\n",
    "\n",
    "\n",
    "1. Reading in the data from the corpus, by opening it, reading it in line by line and counting the syllables and syllable pairs. \n",
    "2. Computing the probabilities for the bigrams (syllable pairs) and unigrams (single syllables)\n",
    "3. Testing the model by comparing the average probability of words to the average probability of non-words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# don't worry about this line\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need a function that reads in the data from the corpusfile. \n",
    "\n",
    "First, we create an empty list to store the results. Then, we open the file for reading, signalled by 'r', and read it in line by line. For each line of in the corpus we remove the end-of-line symbol and add the remained syllable to the list of results. Finally, we return the list with syllables so that it can be used by the rest of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    \"\"\" read corpus from '\\n'-delimited text file\n",
    "    returns a list of syllables \"\"\"\n",
    "\n",
    "    # initialize the return list as an empty list\n",
    "    result = []\n",
    "\n",
    "    # open the file for reading and loop over the lines\n",
    "    for line in open(filename, 'r'):\n",
    "        # get rid of end-of-line ('\\n') symbols \n",
    "        syll = line.strip()\n",
    "        # put syllable at end of list\n",
    "        result.append(syll)\n",
    "\n",
    "    # now that the result list is filled up, return it\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## STEP 2: process_corpus(list_of_syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we need a function that processes the corpus and computes the probabilities for the syllables and syllable-pairs.\n",
    "\n",
    "First, we create two so-called dictionaries for storing the syllables (unigrams) and syllable-pairs (bigrams). A dictionary in Python is a structure in which you can store a string-value along with a linked numerical value. ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_corpus(list_of_syllables):\n",
    "    \"\"\" extract count of uni- & bigram occurrences from sequence\n",
    "    of syllables in list_of_syllables \"\"\"\n",
    "\n",
    "    # dictionaries to hold the counts of the unigrams & bigrams\n",
    "    unigram_dict = {}\n",
    "    bigram_dict = {}\n",
    "\n",
    "    # loop over the indices of list_of_syllables until the first to last element\n",
    "    for syll_idx in range(len(list_of_syllables) - 1):\n",
    "        # form unigram of syllables at index\n",
    "        unigram = (list_of_syllables[syll_idx])\n",
    "\n",
    "        # see if we have already seen this unigram\n",
    "        if unigram in unigram_dict:\n",
    "            # if so, up the count by 1\n",
    "            unigram_dict[unigram] += 1\n",
    "        else:\n",
    "            # if not, set the count to 1\n",
    "            unigram_dict[unigram] = 1\n",
    "\n",
    "        # form bigram of subsequent syllables\n",
    "        bigram = (list_of_syllables[syll_idx], list_of_syllables[syll_idx + 1])\n",
    "        # see if we have already seen this bigram\n",
    "        if bigram in bigram_dict:\n",
    "            # if so, up the count by 1\n",
    "            bigram_dict[bigram] += 1\n",
    "        else:\n",
    "            # if not, set the count to 1\n",
    "            bigram_dict[bigram] = 1\n",
    "\n",
    "    # correct for the off-by-1:\n",
    "    unigram = list_of_syllables[-1]\n",
    "    if unigram in unigram_dict:\n",
    "        unigram_dict[unigram] += 1\n",
    "    else:\n",
    "        unigram_dict[unigram] = 1\n",
    "\n",
    "    # return the dictionaries with the unigram and bigram counts\n",
    "    return unigram_dict, bigram_dict\n",
    "\n",
    "def estimated_bigram_probability(bigram, unigram_dict, bigram_dict):\n",
    "    \"\"\" estimate the probability of bigram (= (syll_1,syll_2)) by:\n",
    "    (count (syll_1,syll_2)) / (count syll_1)\n",
    "    \"\"\"\n",
    "\n",
    "    # set the count to zero\n",
    "    count = 0\n",
    "\n",
    "    # check if bigram is actually in our dict\n",
    "    if bigram in bigram_dict:\n",
    "        # if so, set count to the value from bigram_dict\n",
    "        count = bigram_dict[bigram]\n",
    "\n",
    "    # divide the (bigram) count by the unigram count of the first syllable\n",
    "    # to get the probability\n",
    "    prob = count / unigram_dict[bigram[0]]\n",
    "\n",
    "    # return the calculated probability\n",
    "    return prob\n",
    "\n",
    "def estimated_sequence_probability(list_of_syllables, unigram_dict, bigram_dict):\n",
    "    \"\"\" estimate probability of sequence of syllables,\n",
    "    represented as a list \"\"\"\n",
    "    \n",
    "    # set probability to 1 initially\n",
    "    p = 1.\n",
    "\n",
    "    # loop over sequence indices\n",
    "    for syll_idx in range(len(list_of_syllables) - 1):\n",
    "        # form bigram from subsequent syllables\n",
    "        bigram = (list_of_syllables[syll_idx], list_of_syllables[syll_idx + 1])\n",
    "        \n",
    "        # multiply previous probability with probability of this bigram\n",
    "        p = p * estimated_bigram_probability(bigram, unigram_dict, bigram_dict)\n",
    "\n",
    "    # return the estimated probability of the entire sequence\n",
    "    return p\n",
    "\n",
    "def test_model(unigram_dict, bigram_dict):\n",
    "    \"\"\" test the model on saffran's words and non-words\n",
    "    \"\"\"\n",
    "    \n",
    "    # the words and non-words from saffran\n",
    "    words = [['tu','pi','ro'],\n",
    "             ['go','la','bu'],\n",
    "             ['bi','da','ku'],\n",
    "             ['pa','do','ti']]\n",
    "    non_words = [['da','pi','ku'],\n",
    "                 ['ti','la','do']]\n",
    "\n",
    "    # calculate the sum of the probabilities of the words\n",
    "    sum_words = 0\n",
    "    for word in words:\n",
    "        sum_words += estimated_sequence_probability(word, unigram_dict, bigram_dict)\n",
    "\n",
    "    # divide by the number of words to get the average\n",
    "    average_word = sum_words / len(words)\n",
    "\n",
    "    # idem for the non-words\n",
    "    sum_non_words = 0\n",
    "    for non_word in non_words:\n",
    "        sum_non_words += estimated_sequence_probability(non_word, unigram_dict, bigram_dict)\n",
    "    # divide by the number of words to get the average        \n",
    "    average_non_word = sum_non_words / len(non_words)\n",
    "\n",
    "    print('Average probability for words:', average_word)\n",
    "    print('Average probability for non-words:', average_non_word)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = read_corpus(corfile)\n",
    "    unigram_dict, bigram_dict = process_corpus(corpus)\n",
    "    test_model(unigram_dict, bigram_dict)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
